{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Python api\n",
    "This is example of usage of [kafka-python](https://kafka-python.readthedocs.io/en/master/index.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KafkaProducer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['broker1:1234'])\n",
    "\n",
    "# Asynchronous by default\n",
    "future = producer.send('my-topic', b'raw_bytes')\n",
    "\n",
    "# Block for 'synchronous' sends\n",
    "try:\n",
    "    record_metadata = future.get(timeout=10)\n",
    "except KafkaError:\n",
    "    # Decide what to do if produce request failed...\n",
    "    log.exception()\n",
    "    pass\n",
    "\n",
    "# Successful result returns assigned partition and offset\n",
    "print (record_metadata.topic)\n",
    "print (record_metadata.partition)\n",
    "print (record_metadata.offset)\n",
    "\n",
    "# produce keyed messages to enable hashed partitioning\n",
    "producer.send('my-topic', key=b'foo', value=b'bar')\n",
    "\n",
    "# encode objects via msgpack\n",
    "producer = KafkaProducer(value_serializer=msgpack.dumps)\n",
    "producer.send('msgpack-topic', {'key': 'value'})\n",
    "\n",
    "# produce json messages\n",
    "producer = KafkaProducer(value_serializer=lambda m: json.dumps(m).encode('ascii'))\n",
    "producer.send('json-topic', {'key': 'value'})\n",
    "\n",
    "# produce asynchronously\n",
    "for _ in range(100):\n",
    "    producer.send('my-topic', b'msg')\n",
    "\n",
    "def on_send_success(record_metadata):\n",
    "    print(record_metadata.topic)\n",
    "    print(record_metadata.partition)\n",
    "    print(record_metadata.offset)\n",
    "\n",
    "def on_send_error(excp):\n",
    "    log.error('I am an errback', exc_info=excp)\n",
    "    # handle exception\n",
    "\n",
    "# produce asynchronously with callbacks\n",
    "producer.send('my-topic', b'raw_bytes').add_callback(on_send_success).add_errback(on_send_error)\n",
    "\n",
    "# block until all async messages are sent\n",
    "producer.flush()\n",
    "\n",
    "# configure multiple retries\n",
    "producer = KafkaProducer(retries=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# To consume latest messages and auto-commit offsets\n",
    "consumer = KafkaConsumer('example_event',\n",
    "                         group_id='my-group',\n",
    "                         bootstrap_servers=['localhost:9092'])\n",
    "for message in consumer:\n",
    "    # message value and key are raw bytes -- decode if necessary!\n",
    "    # e.g., for unicode: `message.value.decode('utf-8')`\n",
    "    print (\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition,\n",
    "                                          message.offset, message.key,\n",
    "                                          message.value))\n",
    "\n",
    "# consume earliest available messages, don't commit offsets\n",
    "KafkaConsumer(auto_offset_reset='earliest', enable_auto_commit=False)\n",
    "\n",
    "# consume json messages\n",
    "KafkaConsumer(value_deserializer=lambda m: json.loads(m.decode('ascii')))\n",
    "\n",
    "# consume msgpack\n",
    "KafkaConsumer(value_deserializer=msgpack.unpackb)\n",
    "\n",
    "# StopIteration if no message after 1sec\n",
    "KafkaConsumer(consumer_timeout_ms=1000)\n",
    "\n",
    "# Subscribe to a regex topic pattern\n",
    "consumer = KafkaConsumer()\n",
    "consumer.subscribe(pattern='^awesome.*')\n",
    "\n",
    "# Use multiple consumers in parallel w/ 0.9 kafka brokers\n",
    "# typically you would run each on a different server / process / CPU\n",
    "consumer1 = KafkaConsumer('my-topic',\n",
    "                          group_id='my-group',\n",
    "                          bootstrap_servers='my.server.com')\n",
    "consumer2 = KafkaConsumer('my-topic',\n",
    "                          group_id='my-group',\n",
    "                          bootstrap_servers='my.server.com')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
